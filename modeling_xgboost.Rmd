---
title: 'Titanic: Modeling with XGBoost'
author: "Aleksei Neverov"
date: '16 февраля 2018 г '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(mlr)
library(rJava)
library(FSelector)
library(dummies)
library(caret)
library(rpart)
library(xgboost)
```

## Data Reading
```{r}
# Load training and test datasets
training <- read.csv("train.csv", header = TRUE)
testing  <- read.csv("test.csv", header = TRUE)
training_res <- training$Survived
testing_ids <- testing$PassengerId
```

```{r}
summary(training)
```

## Imputing NAs
```{r}
imp_train <- impute(training, classes = list(factor = imputeMode(), integer = imputeMean(), numeric = imputeMean()), dummy.classes = c("integer","factor"), dummy.type = "numeric")

imp_test <- impute(testing, classes = list(factor = imputeMode(), integer = imputeMean(), numeric = imputeMean()), dummy.classes = c("integer","factor"), dummy.type = "numeric")

training <- imp_train$data
testing  <- imp_test$data
```

```{r}
training <- training %>%
  mutate(Deck = as.factor(substr(as.character(Cabin), 0, 1)),
         HasCabin = as.factor(grepl("[0-9]+", Cabin)),
         IsFamily = as.factor(SibSp + Parch > 0)) %>%
  dplyr::select(-PassengerId, -Name, -Ticket, -Cabin, -Survived)

testing <- testing %>%
  mutate(Deck = as.factor(substr(as.character(Cabin), 0, 1)),
         HasCabin = as.factor(grepl("[0-9]+", Cabin)),
         IsFamily = as.factor(SibSp + Parch > 0)) %>%
  dplyr::select(-PassengerId, -Name, -Ticket, -Cabin)

training <- dummy.data.frame(training)
testing  <- dummy.data.frame(testing)

training_dum <- training
testing_dum <- testing


testing <- mutate(testing, Embarked = 0, DeckT = 0)

testing <- testing[,names(training)]

# training$Survived <- as.factor(training$Survived)
# testing  <- testing %>% mutate(Survived = factor(0, levels = c("0","1")))

```

## Machine learning

```{r}
set.seed(123)


train_xgb_matrix = xgb.DMatrix(data = as.matrix(training), label = training_res)
test_xgb_matrix = train_xgb_matrix

watchlist <- list(train = train_xgb_matrix, test=test_xgb_matrix)


searchGrid = expand.grid(subsample = c(.5, .75, 1),
                         colsample_bytree = c(0.6, 0.8, 1),
                         ntrees = seq(300,400, by = 20),
                         nfolds = 3:10,
                         learning_rate = seq(.05, .09, by = .01),
                         depth = 20:30)

rmseHyperParams <- apply(searchGrid, 1, function(parameterList){
  curSubSample <- parameterList[["subsample"]]
  curColSample <- parameterList[["colsample_bytree"]]
  ntrees       <- parameterList[["ntrees"]]
  nfolds       <- parameterList[["nfolds"]]
  rate         <- parameterList[["learning_rate"]]
  depth        <- parameterList[["depth"]]
  
  xgbcv <- xgb.cv(data =  train_xgb_matrix, nrounds = ntrees, nfold = nfolds, showsd = TRUE, 
                          metrics = "error", verbose = TRUE, "eval_metric" = "error", "booster" = "gbtree",
                          "objective" = "binary:logistic", "max.depth" = depth, "eta" = rate,                               
                          "subsample" = curSubSample, "colsample_bytree" = curColSample
                  )
  
  xvalidationScores <- as.data.frame(xgbcv)
  
  rmse <- tail(xvalidationScores$test.rmse.mean, 1)
  error <- tail(xvalidationScores$test.error.mean, 1)

  return(c(rmse, error, curSubSample, curColSample, ntrees, nfolds, rate, depth))

})


bst <- xgb.train (params = params, 
                  data = train_xgb_matrix, 
                  nrounds = 100, 
                  watchlist = watchlist, 
                  print.every.n = 10, 
                  early.stop.round = 100, 
                  maximize = F , 
                  eval_metric = "error")

prediction <- as.numeric(predict(bst, as.matrix(dplyr::select(testing, -Survived))) > .5)
res <- data.frame(PassengerId = testing_ids, Survived = prediction)

write.csv(res, "submission3.csv", sep=",", row.names = FALSE)
```