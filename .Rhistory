summarizeColumns(training)
summarizeColumns(testing)
set.seed(123)
inTrain <- createDataPartition(training$Survived, p = .6, list = FALSE)
validation <- training[-inTrain,]
training   <- training[inTrain,]
trainTask <- makeClassifTask(data = training, target = "Survived", positive = "1")
validTask <- makeClassifTask(data = validation, target = "Survived", positive = "1")
testTask  <- makeClassifTask(data = testing, target = "Survived", positive = "1")
im_feat <- generateFilterValuesData(trainTask, method = c("information.gain","chi.squared"))
plotFilterValues(im_feat,n.show = 20)
im_fit
im_feat <- generateFilterValuesData(trainTask, method = c("information.gain","chi.squared"))
im_feat
top_task <- filterFeatures(trainTask, method = "rf.importance", abs = 6)
install.packages("randomForestSRC")
top_task <- filterFeatures(trainTask, method = "rf.importance", abs = 6)
getParamSet("classif.randomForest")
#create a learner
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(ntree = 200, mtry = 3))
rf$par.vals <- list(
importance = TRUE
)
#set tunable parameters
#grid search to find hyperparameters
rf_param <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 3, upper = 10),
makeIntegerParam("nodesize", lower = 10, upper = 50)
)
#let's do random search for 50 iterations
rancontrol <- makeTuneControlRandom(maxit = 50L)
#set 3 fold cross validation
set_cv <- makeResampleDesc("CV",iters = 3L)
#hypertuning
rf_tune <- tuneParams(learner = rf, resampling = set_cv, task = trainTask, par.set = rf_param, control = rancontrol, measures = acc, show.info = FALSE)
#hypertuning
rf_tune <- tuneParams(learner = rf, resampling = set_cv, task = top_task, par.set = rf_param, control = rancontrol, measures = acc, show.info = FALSE)
getParamSet("classif.randomForest")
#create a learner
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(ntree = 200, mtry = 3))
rf$par.vals <- list(
importance = TRUE
)
#set tunable parameters
#grid search to find hyperparameters
rf_param <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 3, upper = 6),
makeIntegerParam("nodesize", lower = 10, upper = 50)
)
#let's do random search for 50 iterations
rancontrol <- makeTuneControlRandom(maxit = 50L)
#set 3 fold cross validation
set_cv <- makeResampleDesc("CV",iters = 3L)
#hypertuning
rf_tune <- tuneParams(learner = rf, resampling = set_cv, task = top_task, par.set = rf_param, control = rancontrol, measures = acc, show.info = FALSE)
rf.tree <- setHyperPars(rf, par.vals = rf_tune$x)
#train a model
rforest <- mlr::train(rf.tree, top_task)
#make predictions
rfmodel <- predict(rforest, validTask)
confusionMatrix(rfmodel$data$response, validation$Survived)
rfmodel <- predict(rforest, testTask)
library(nnet)
knitr::opts_chunk$set(echo = TRUE)
# Load training and test datasets
training <- read.csv("train.csv", header = TRUE)
testing  <- read.csv("test.csv", header = TRUE)
## Imputing NAs
imp_train <- impute(training, classes = list(factor = imputeMode(), integer = imputeMean(), numeric = imputeMean()), dummy.classes = c("integer","factor"), dummy.type = "numeric")
imp_test <- impute(testing, classes = list(factor = imputeMode(), integer = imputeMean(), numeric = imputeMean()), dummy.classes = c("integer","factor"), dummy.type = "numeric")
training <- imp_train$data
testing  <- imp_test$data
training <- training %>%
mutate(Deck = as.factor(substr(as.character(Cabin), 0, 1)),
HasCabin = as.factor(grepl("[0-9]+", Cabin)),
IsFamily = as.factor(SibSp + Parch > 0)) %>%
dplyr::select(-PassengerId, -Name, -Ticket, -Cabin)
testing <- testing %>%
mutate(Deck = as.factor(substr(as.character(Cabin), 0, 1)),
HasCabin = as.factor(grepl("[0-9]+", Cabin)),
IsFamily = as.factor(SibSp + Parch > 0)) %>%
dplyr::select( -Name, -Ticket, -Cabin)
training <- dummy.data.frame(training)
testing  <- dummy.data.frame(testing)
testing_ids <- testing$PassengerId
training_dum <- training
testing_dum <- testing
training$Survived <- as.factor(training$Survived)
testing  <- testing %>% mutate(Survived = factor(0, levels = c("0","1")))
set.seed(123)
inTrain <- createDataPartition(training$Survived, p = .6, list = FALSE)
validation <- training[-inTrain,]
training   <- training[inTrain,]
nn_model <- nnet(Survided ~ ., data = training)
nn_model <- nnet(Survived ~ ., data = training)
nn_model <- nnet(Survived ~ ., data = training, size = 2)
plot(nn_model)
nn_predict <- predict(nn_model, newdata = validation)
confusionMatrix(nn_predict, validation$Survived)
nn_predict
# Load training and test datasets
training <- read.csv("train.csv", header = TRUE)
testing  <- read.csv("test.csv", header = TRUE)
## Imputing NAs
imp_train <- impute(training, classes = list(factor = imputeMode(), integer = imputeMean(), numeric = imputeMean()), dummy.classes = c("integer","factor"), dummy.type = "numeric")
imp_test <- impute(testing, classes = list(factor = imputeMode(), integer = imputeMean(), numeric = imputeMean()), dummy.classes = c("integer","factor"), dummy.type = "numeric")
training <- imp_train$data
testing  <- imp_test$data
training <- training %>%
mutate(Deck = as.factor(substr(as.character(Cabin), 0, 1)),
HasCabin = as.factor(grepl("[0-9]+", Cabin)),
IsFamily = as.factor(SibSp + Parch > 0)) %>%
dplyr::select(-PassengerId, -Name, -Ticket, -Cabin)
testing <- testing %>%
mutate(Deck = as.factor(substr(as.character(Cabin), 0, 1)),
HasCabin = as.factor(grepl("[0-9]+", Cabin)),
IsFamily = as.factor(SibSp + Parch > 0)) %>%
dplyr::select( -Name, -Ticket, -Cabin)
training <- dummy.data.frame(training)
testing  <- dummy.data.frame(testing)
testing_ids <- testing$PassengerId
training$Survived <- as.factor(training$Survived)
testing  <- testing %>% mutate(Survived = factor(0, levels = c("0","1")))
set.seed(123)
inTrain <- createDataPartition(training$Survived, p = .6, list = FALSE)
validation <- training[-inTrain,]
training   <- training[inTrain,]
nn_model <- nnet(Survived ~ ., data = training, size = 2)
plot(nn_model)
nn_predict <- predict(nn_model, newdata = validation)
View(nn_predict)
nn_model <- nnet(Survived ~ ., data = training, size = 0)
round(nrow(training) * .3)
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) * .3))
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 30))
nn_predict <- predict(nn_model, newdata = validation)
View(nn_predict)
nn_model <- nnet(as.factor(Survived) ~ ., data = training, size = round(nrow(training) / 30))
nn_predict <- predict(nn_model, newdata = validation)
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 30))
nn_predict <- predict(nn_model, newdata = validation, type = "class")
nn_predict
class(nn_predict)
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 30))
nn_predict <- factor(predict(nn_model, newdata = validation, type = "class"), levels = training$Survived)
nn_predict <- factor(predict(nn_model, newdata = validation, type = "class"))
confusionMatrix(nn_predict, validation$Survived)
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 30),
rang=(1/max(filter(training, -Survived))), decay=5e-4, maxit=200)
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 30),
rang=(1/max(dplyr::select(training, -Survived))), decay=5e-4, maxit=200)
nn_predict <- factor(predict(nn_model, newdata = validation, type = "class"))
confusionMatrix(nn_predict, validation$Survived)
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 30),
rang=(1/max(dplyr::select(training, -Survived))), decay=5e-4, maxit=300)
nn_predict <- factor(predict(nn_model, newdata = validation, type = "class"))
confusionMatrix(nn_predict, validation$Survived)
cm <- confusionMatrix(nn_predict, validation$Survived)
cm$overall$Accuracy
for (i in seq(10,300, by = 10)) {
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 30),
rang=(1/max(dplyr::select(training, -Survived))), decay=5e-4, maxit=i,
)
nn_predict <- factor(nnet::predict(nn_model, newdata = validation, type = "class"))
cm <- confusionMatrix(nn_predict, validation$Survived)
print(paste("i",i,"Acc =",cm$overall[1], sep="\t"))
}
for (i in seq(10,300, by = 10)) {
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 30),
rang=(1/max(dplyr::select(training, -Survived))), decay=5e-4, maxit=i,
varbose = FALSE)
nn_predict <- factor(predict(nn_model, newdata = validation, type = "class"))
cm <- confusionMatrix(nn_predict, validation$Survived)
print(paste("i",i,"Acc =",cm$overall[1], sep="\t"))
}
acc = data.frame()
names(acc) <- c("NumIter","Acc")
acc = data.frame(NumerIter = 0, Acc = 0)
for (i in seq(10,300, by = 10)) {
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 30),
rang=(1/max(dplyr::select(training, -Survived))), decay=5e-4, maxit=i,
varbose = FALSE)
nn_predict <- factor(predict(nn_model, newdata = validation, type = "class"))
cm <- confusionMatrix(nn_predict, validation$Survived)
acc <- rbind(acc, c(i, acc))
}
acc = data.frame(NumerIter = 0, Acc = 0)
for (i in seq(10,300, by = 10)) {
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 30),
rang=(1/max(dplyr::select(training, -Survived))), decay=5e-4, maxit=i,
varbose = FALSE)
nn_predict <- factor(predict(nn_model, newdata = validation, type = "class"))
cm <- confusionMatrix(nn_predict, validation$Survived)
acc <- rbind(acc, data.frame(i, acc))
}
acc = data.frame(NumerIter = 0, Acc = 0)
for (i in seq(10,300, by = 10)) {
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 30),
rang=(1/max(dplyr::select(training, -Survived))), decay=5e-4, maxit=i,
varbose = FALSE)
nn_predict <- factor(predict(nn_model, newdata = validation, type = "class"))
cm <- confusionMatrix(nn_predict, validation$Survived)
acc <- rbind(acc, data.frame(NumerIter = i, Acc = acc))
}
acc <- rbind(acc,
data.frame(NumerIter = i, Acc = acc)
data.frame(NumerIter = i, Acc = acc)
data.frame(NumerIter = i, Acc = acc)
acc = data.frame(NumerIter = 0, Acc = 0)
for (i in seq(10,300, by = 10)) {
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 30),
rang=(1/max(dplyr::select(training, -Survived))), decay=5e-4, maxit=i,
varbose = FALSE)
nn_predict <- factor(predict(nn_model, newdata = validation, type = "class"))
cm <- confusionMatrix(nn_predict, validation$Survived)
acc <- rbind(acc, data.frame(NumerIter = i, Acc = cm$overall[1]))
}
plot(acc)
plot(acc, type = "l")
# Load training and test datasets
training <- read.csv("train.csv", header = TRUE)
training$Survived <- as.factor(training$Survived)
set.seed(123)
inTrain <- createDataPartition(training$Survived, p = .6, list = FALSE)
validation <- training[-inTrain,]
training   <- training[inTrain,]
acc = data.frame(NumerIter = 0, Acc = 0)
for (i in seq(10,300, by = 10)) {
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 30),
rang=(1/max(dplyr::select(training, -Survived))), decay=5e-4, maxit=i,
varbose = FALSE)
nn_predict <- factor(predict(nn_model, newdata = validation, type = "class"))
cm <- confusionMatrix(nn_predict, validation$Survived)
acc <- rbind(acc, data.frame(NumerIter = i, Acc = cm$overall[1]))
}
acc = data.frame(NumerIter = 0, Acc = 0)
for (i in seq(10,300, by = 10)) {
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 100),
rang=(1/max(dplyr::select(training, -Survived))), decay=5e-4, maxit=i,
varbose = FALSE)
nn_predict <- factor(predict(nn_model, newdata = validation, type = "class"))
cm <- confusionMatrix(nn_predict, validation$Survived)
acc <- rbind(acc, data.frame(NumerIter = i, Acc = cm$overall[1]))
}
acc = data.frame(NumerIter = 0, Acc = 0)
for (i in seq(10,300, by = 10)) {
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 300),
rang=(1/max(dplyr::select(training, -Survived))), decay=5e-4, maxit=i,
varbose = FALSE)
nn_predict <- factor(predict(nn_model, newdata = validation, type = "class"))
cm <- confusionMatrix(nn_predict, validation$Survived)
acc <- rbind(acc, data.frame(NumerIter = i, Acc = cm$overall[1]))
}
acc = data.frame(NumerIter = 0, Acc = 0)
for (i in seq(10,300, by = 10)) {
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 600),
rang=(1/max(dplyr::select(training, -Survived))), decay=5e-4, maxit=i,
varbose = FALSE)
nn_predict <- factor(predict(nn_model, newdata = validation, type = "class"))
cm <- confusionMatrix(nn_predict, validation$Survived)
acc <- rbind(acc, data.frame(NumerIter = i, Acc = cm$overall[1]))
}
# Load training and test datasets
training <- read.csv("train.csv", header = TRUE)
testing  <- read.csv("test.csv", header = TRUE)
training <- training %>%
mutate(Deck = as.factor(substr(as.character(Cabin), 0, 1)),
HasCabin = as.factor(grepl("[0-9]+", Cabin)),
IsFamily = as.factor(SibSp + Parch > 0)) %>%
dplyr::select(-PassengerId, -Name, -Ticket, -Cabin)
training$Survived <- as.factor(training$Survived)
set.seed(123)
inTrain <- createDataPartition(training$Survived, p = .6, list = FALSE)
validation <- training[-inTrain,]
training   <- training[inTrain,]
acc = data.frame(NumerIter = 0, Acc = 0)
for (i in seq(10,300, by = 10)) {
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 30),
rang=(1/max(dplyr::select(training, -Survived))), decay=5e-4, maxit=i,
varbose = FALSE)
nn_predict <- factor(predict(nn_model, newdata = validation, type = "class"))
cm <- confusionMatrix(nn_predict, validation$Survived)
acc <- rbind(acc, data.frame(NumerIter = i, Acc = cm$overall[1]))
}
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 30))
nn_predict <- factor(predict(nn_model, newdata = validation, type = "class"))
cm <- confusionMatrix(nn_predict, validation$Survived)
acc <- rbind(acc, data.frame(NumerIter = i, Acc = cm$overall[1]))
acc = data.frame(NumerIter = 0, Acc = 0)
for (i in seq(10,300, by = 10)) {
nn_model <- nnet(Survived ~ ., data = training, size = round(nrow(training) / 30))
# ,
#                    rang=(1/max(dplyr::select(training, -Survived))), decay=5e-4, maxit=i,
#                varbose = FALSE)
nn_predict <- factor(predict(nn_model, newdata = validation, type = "class"))
cm <- confusionMatrix(nn_predict, validation$Survived)
acc <- rbind(acc, data.frame(NumerIter = i, Acc = cm$overall[1]))
}
plot(acc, type = "l", color = "blue")
View(acc)
# Load training and test datasets
training <- read.csv("train.csv", header = TRUE)
testing  <- read.csv("test.csv", header = TRUE)
imp_train <- impute(training, classes = list(factor = imputeMode(), integer = imputeMean(), numeric = imputeMean()), dummy.classes = c("integer","factor"), dummy.type = "numeric")
imp_test <- impute(testing, classes = list(factor = imputeMode(), integer = imputeMean(), numeric = imputeMean()), dummy.classes = c("integer","factor"), dummy.type = "numeric")
training <- imp_train$data
testing  <- imp_test$data
training <- training %>%
mutate(Deck = as.factor(substr(as.character(Cabin), 0, 1)),
HasCabin = as.factor(grepl("[0-9]+", Cabin)),
IsFamily = as.factor(SibSp + Parch > 0)) %>%
dplyr::select(-PassengerId, -Name, -Ticket, -Cabin)
testing <- testing %>%
mutate(Deck = as.factor(substr(as.character(Cabin), 0, 1)),
HasCabin = as.factor(grepl("[0-9]+", Cabin)),
IsFamily = as.factor(SibSp + Parch > 0)) %>%
dplyr::select( -Name, -Ticket, -Cabin)
training <- dummy.data.frame(training)
testing  <- dummy.data.frame(testing)
testing_ids <- testing$PassengerId
training_dum <- training
testing_dum <- testing
training$Survived <- as.factor(training$Survived)
testing  <- testing %>% mutate(Survived = factor(0, levels = c("0","1")))
summarizeColumns(training)
summarizeColumns(testing)
set.seed(123)
inTrain <- createDataPartition(training$Survived, p = .6, list = FALSE)
validation <- training[-inTrain,]
training   <- training[inTrain,]
trainTask <- makeClassifTask(data = training, target = "Survived", positive = "1")
validTask <- makeClassifTask(data = validation, target = "Survived", positive = "1")
im_feat <- generateFilterValuesData(trainTask, method = c("information.gain","chi.squared"))
plotFilterValues(im_feat,n.show = 20)
top_task <- filterFeatures(trainTask, method = "rf.importance", abs = 10)
top_task <- filterFeatures(trainTask, method = "rf.importance", abs = 10)
getParamSet("classif.randomForest")
#create a learner
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(ntree = 200, mtry = 3))
rf$par.vals <- list(
importance = TRUE
)
#set tunable parameters
#grid search to find hyperparameters
rf_param <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 3, upper = 6),
makeIntegerParam("nodesize", lower = 10, upper = 50)
)
#let's do random search for 50 iterations
rancontrol <- makeTuneControlRandom(maxit = 50L)
#set 3 fold cross validation
set_cv <- makeResampleDesc("CV",iters = 3L)
#hypertuning
rf_tune <- tuneParams(learner = rf, resampling = set_cv, task = top_task, par.set = rf_param, control = rancontrol, measures = acc, show.info = FALSE)
getParamSet("classif.randomForest")
#create a learner
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(ntree = 200, mtry = 3))
rf$par.vals <- list(
importance = TRUE
)
#set tunable parameters
#grid search to find hyperparameters
rf_param <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 3, upper = 6),
makeIntegerParam("nodesize", lower = 10, upper = 50)
)
#let's do random search for 50 iterations
rancontrol <- makeTuneControlRandom(maxit = 50L)
#set 3 fold cross validation
set_cv <- makeResampleDesc("CV",iters = 3L)
#hypertuning
rf_tune <- tuneParams(learner = rf, resampling = set_cv, task = top_task, par.set = rf_param, control = rancontrol, measures = acc, show.info = FALSE)
rm(list = ls())
# Load training and test datasets
training <- read.csv("train.csv", header = TRUE)
testing  <- read.csv("test.csv", header = TRUE)
imp_train <- impute(training, classes = list(factor = imputeMode(), integer = imputeMean(), numeric = imputeMean()), dummy.classes = c("integer","factor"), dummy.type = "numeric")
imp_test <- impute(testing, classes = list(factor = imputeMode(), integer = imputeMean(), numeric = imputeMean()), dummy.classes = c("integer","factor"), dummy.type = "numeric")
training <- imp_train$data
testing  <- imp_test$data
training <- training %>%
mutate(Deck = as.factor(substr(as.character(Cabin), 0, 1)),
HasCabin = as.factor(grepl("[0-9]+", Cabin)),
IsFamily = as.factor(SibSp + Parch > 0)) %>%
dplyr::select(-PassengerId, -Name, -Ticket, -Cabin)
testing <- testing %>%
mutate(Deck = as.factor(substr(as.character(Cabin), 0, 1)),
HasCabin = as.factor(grepl("[0-9]+", Cabin)),
IsFamily = as.factor(SibSp + Parch > 0)) %>%
dplyr::select( -Name, -Ticket, -Cabin)
training <- dummy.data.frame(training)
testing  <- dummy.data.frame(testing)
testing_ids <- testing$PassengerId
training_dum <- training
testing_dum <- testing
training$Survived <- as.factor(training$Survived)
testing  <- testing %>% mutate(Survived = factor(0, levels = c("0","1")))
summarizeColumns(training)
summarizeColumns(testing)
set.seed(123)
inTrain <- createDataPartition(training$Survived, p = .6, list = FALSE)
validation <- training[-inTrain,]
training   <- training[inTrain,]
trainTask <- makeClassifTask(data = training, target = "Survived", positive = "1")
validTask <- makeClassifTask(data = validation, target = "Survived", positive = "1")
trainTask <- normalizeFeatures(trainTask,method = "standardize")
validTask <- normalizeFeatures(validTask,method = "standardize")
im_feat <- generateFilterValuesData(trainTask, method = c("information.gain","chi.squared"))
plotFilterValues(im_feat,n.show = 20)
top_task <- filterFeatures(trainTask, method = "rf.importance", abs = 10)
getParamSet("classif.randomForest")
#create a learner
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(ntree = 200, mtry = 3))
rf$par.vals <- list(
importance = TRUE
)
#set tunable parameters
#grid search to find hyperparameters
rf_param <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 3, upper = 6),
makeIntegerParam("nodesize", lower = 10, upper = 50)
)
#let's do random search for 50 iterations
rancontrol <- makeTuneControlRandom(maxit = 50L)
#set 3 fold cross validation
set_cv <- makeResampleDesc("CV",iters = 3L)
#hypertuning
rf_tune <- tuneParams(learner = rf, resampling = set_cv, task = top_task, par.set = rf_param, control = rancontrol, measures = acc, show.info = FALSE)
rf.tree <- setHyperPars(rf, par.vals = rf_tune$x)
#train a model
rforest <- mlr::train(rf.tree, top_task)
#make predictions
rfmodel <- predict(rforest, validTask)
confusionMatrix(rfmodel$data$response, validation$Survived)
# rfmodel <- predict(rforest, testTask)
xg_tune$y
# Load training and test datasets
training <- read.csv("train.csv", header = TRUE)
testing  <- read.csv("test.csv", header = TRUE)
imp_train <- impute(training, classes = list(factor = imputeMode(), integer = imputeMean(), numeric = imputeMean()), dummy.classes = c("integer","factor"), dummy.type = "numeric")
imp_test <- impute(testing, classes = list(factor = imputeMode(), integer = imputeMean(), numeric = imputeMean()), dummy.classes = c("integer","factor"), dummy.type = "numeric")
training <- imp_train$data
testing  <- imp_test$data
training <- training %>%
mutate(Deck = as.factor(substr(as.character(Cabin), 0, 1)),
HasCabin = as.factor(grepl("[0-9]+", Cabin)),
IsFamily = as.factor(SibSp + Parch > 0)) %>%
dplyr::select(-PassengerId, -Name, -Ticket, -Cabin)
testing <- testing %>%
mutate(Deck = as.factor(substr(as.character(Cabin), 0, 1)),
HasCabin = as.factor(grepl("[0-9]+", Cabin)),
IsFamily = as.factor(SibSp + Parch > 0)) %>%
dplyr::select( -Name, -Ticket, -Cabin)
training <- dummy.data.frame(training)
testing  <- dummy.data.frame(testing)
testing_ids <- testing$PassengerId
training_dum <- training
testing_dum <- testing
training$Survived <- as.factor(training$Survived)
testing  <- testing %>% mutate(Survived = factor(0, levels = c("0","1")))
summarizeColumns(training)
summarizeColumns(testing)
set.seed(123)
inTrain <- createDataPartition(training$Survived, p = .6, list = FALSE)
validation <- training[-inTrain,]
training   <- training[inTrain,]
trainTask <- makeClassifTask(data = training, target = "Survived", positive = "1")
validTask <- makeClassifTask(data = validation, target = "Survived", positive = "1")
# testTask  <- makeClassifTask(data = testing, target = "Survived", positive = "1")
#load xgboost
set.seed(1001)
getParamSet("classif.xgboost")
#make learner with inital parameters
xg_set <- makeLearner("classif.xgboost", predict.type = "response")
xg_set$par.vals <- list(
objective = "binary:logistic",
eval_metric = "error",
nrounds = 250
)
#define parameters for tuning
xg_ps <- makeParamSet(
makeIntegerParam("nrounds",lower=200,upper=600),
makeIntegerParam("max_depth",lower=3,upper=20),
makeNumericParam("lambda",lower=0.55,upper=0.60),
makeNumericParam("eta", lower = 0.001, upper = 0.5),
makeNumericParam("subsample", lower = 0.10, upper = 0.80),
makeNumericParam("min_child_weight",lower=1,upper=5),
makeNumericParam("colsample_bytree",lower = 0.2,upper = 0.8)
)
#define search function
rancontrol <- makeTuneControlRandom(maxit = 100L) #do 100 iterations
#3 fold cross validation
set_cv <- makeResampleDesc("CV",iters = 3L)
#tune parameters
xg_tune <- tuneParams(learner = xg_set, task = trainTask, resampling = set_cv,measures = acc,par.set = xg_ps, control = rancontrol, show.info = FALSE)
#set parameters
xg_new <- setHyperPars(learner = xg_set, par.vals = xg_tune$x)
#train model
xgmodel <- mlr::train(xg_new, trainTask)
#test model
predict.xg <- predict(xgmodel, validTask)
confusionMatrix(predict.xg$data$response, validation$Survived)
#submission file
# submit <- data.frame(Loan_ID = test$Loan_ID, Loan_Status = predict.xg$data$response)
#write.csv(submit, "submit7.csv",row.names = F)
xg_tune$x
xg_tune$y
